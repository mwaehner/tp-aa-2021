{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba129231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8e7c9a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       aaron  abandon  abandoned  abandoning  abbas  abc  abdel  abdullah  \\\n",
      "0          0        0          0           0      0    0      0         0   \n",
      "1          0        0          0           0      0    0      1         0   \n",
      "2          1        0          0           0      0    0      0         0   \n",
      "3          0        0          0           0      0    0      0         0   \n",
      "4          0        0          0           0      0    0      0         0   \n",
      "...      ...      ...        ...         ...    ...  ...    ...       ...   \n",
      "44893      0        0          0           0      0    0      0         0   \n",
      "44894      0        0          0           0      0    0      0         0   \n",
      "44895      0        0          1           0      0    0      0         0   \n",
      "44896      0        0          0           0      0    0      0         0   \n",
      "44897      0        0          0           0      0    0      0         0   \n",
      "\n",
      "       abe  abedin  ...  younger  youre  youth  youths  youtube  zealand  \\\n",
      "0        0       0  ...        0      0      0       0        0        0   \n",
      "1        0       0  ...        0      0      0       0        0        0   \n",
      "2        0       0  ...        0      0      0       0        0        0   \n",
      "3        0       0  ...        0      0      0       0        0        0   \n",
      "4        0       0  ...        0      0      0       0        0        0   \n",
      "...    ...     ...  ...      ...    ...    ...     ...      ...      ...   \n",
      "44893    0       0  ...        0      0      0       0        0        0   \n",
      "44894    0       0  ...        0      0      0       0        0        0   \n",
      "44895    0       0  ...        0      0      0       0        0        0   \n",
      "44896    0       0  ...        0      0      0       0        0        0   \n",
      "44897    0       0  ...        0      0      0       0        0        0   \n",
      "\n",
      "       zero  zimbabwe  zone  zones  \n",
      "0         0         0     0      0  \n",
      "1         0         0     0      0  \n",
      "2         0         0     0      0  \n",
      "3         0         0     0      0  \n",
      "4         0         0     0      0  \n",
      "...     ...       ...   ...    ...  \n",
      "44893     0         0     0      0  \n",
      "44894     0         0     0      0  \n",
      "44895     0         0     0      0  \n",
      "44896     0         0     0      0  \n",
      "44897     0         0     0      0  \n",
      "\n",
      "[44898 rows x 6888 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\"\"\" #1: Levantamo el datacet \"\"\"\n",
    "fake = pd.read_csv(\"../../archive/Fake.csv\")\n",
    "true = pd.read_csv(\"../../archive/True.csv\")\n",
    "\n",
    "# ponemos los dos en uno\n",
    "fake[\"label\"] = 1\n",
    "true[\"label\"] = 0\n",
    "df = pd.concat([fake, true], ignore_index = True)\n",
    "\n",
    "#print(df.text)\n",
    "\n",
    "\"\"\" #2: Limpiamo el tecsto \"\"\"\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    # agregar otras...\n",
    "    return text\n",
    "\n",
    "df.title = df.title.map(clean_text)\n",
    "df.text = df.text.map(clean_text)\n",
    "\n",
    "\"\"\" #3: Vectorizamos el texto \"\"\"\n",
    "\n",
    "# removemos palabras con muy alta o muy baja frecuencia. ademas, removemos las \"stop words\" del inglés\n",
    "# (palabras como 'the', 'a', 'he', 'her', etc.)\n",
    "MAX_FREQ_THRESHOLD = 0.8\n",
    "MIN_FREQ_THRESHOLD = 0.003\n",
    "\n",
    "# chusmear https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "cv = CountVectorizer(\n",
    "    stop_words='english',\n",
    "    max_df=MAX_FREQ_THRESHOLD,\n",
    "    min_df=MIN_FREQ_THRESHOLD  # seguro hay mas parametros piolas para usar\n",
    ")\n",
    "data_cv = cv.fit_transform(df.text)\n",
    "# print(cv.get_feature_names())  # vocabulario\n",
    "# print(len(cv.get_feature_names())) # tamaño del vocabulario\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = df.index\n",
    "\n",
    "print(data_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3d63ab42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "44893    0\n",
       "44894    0\n",
       "44895    0\n",
       "44896    0\n",
       "44897    0\n",
       "Name: label, Length: 44898, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "410f6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "790fbeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_dtm.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3bb9e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f3def049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 31428\n",
      "Number of testing examples: 13470\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {X_train.shape[0]}\")\n",
    "print(f\"Number of testing examples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ae6c0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9407cbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ccd71fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.996807720861173\n"
     ]
    }
   ],
   "source": [
    "score = lr.score(X_test, Y_test)\n",
    "print(f\"Accuracy: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc64be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "\n",
    "- Probar con otros valores (idealmente no arbitrarios) de MIN_FREQUENCY y MAX_FREQUENCY\n",
    "- Hacer grafiquitos lindos\n",
    "- Probar usando solo los títulos de la noticia, ignorar el texto.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
